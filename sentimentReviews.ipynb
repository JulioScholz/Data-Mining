{"cells":[{"cell_type":"markdown","metadata":{},"source":["Aluno: Júlio Scholz\n","\n","Como features, fiz uma análise de sentimentos nas reviews dos estabelecimentos, agrupei os pela média de cada estabelicimento e utilizei os seguintes classificadores:<br>\n","Knn, NaiveBayes, Decision Tree, Rede neural e Ada Boost <br> \n","\n","O melhor classificador foi o KNeighbors com 30 vizinhos, com um score de 81%, no kaggle ele resultou Public Score de 0.82. <br>\n","Utilizei as features: neg, pos, compound <br>\n","Neg é o quão negativa uma review é no intervalo [0 a 1] <br>\n","Pos é o quão positiva uma review é no intervalo [0 a 1] <br>\n","Compound é a pontuação agregada entre a positividade, negatividade e a neutralidade no intervalo [-1 a 1] <br>\n","<br>\n","Também testei um classificador de rede neural e o Ada Boost, ambos com resultados muito similares ao KNeighbors. <br>\n","Testei uma estratégia de votação por maioria utilizando a rede neural o Ada Boost e KNeighbors, e o score foi 0.01 menor que o KNeighbors sozinho."]},{"cell_type":"code","execution_count":211,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","import string\n","import re\n","\n","from collections import Counter\n","from nltk.corpus import stopwords\n","from nltk.stem.porter import PorterStemmer\n","from nltk.sentiment import SentimentIntensityAnalyzer\n","\n","from sklearn import tree\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.model_selection import cross_val_score\n","from sklearn.ensemble import AdaBoostClassifier\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.model_selection import GridSearchCV"]},{"cell_type":"code","execution_count":212,"metadata":{},"outputs":[],"source":["#Carregando as bases de dado pra memória\n","\n","revTrain = pd.read_csv(\"reviewsTrainToronto.csv\")\n","revTrain.drop('user_id',axis=1,inplace=True)\n","revTrain.drop('date',axis=1,inplace=True)\n","revTrain\n","\n","revTest = pd.read_csv(\"reviewsTestToronto.csv\")\n","revTest.drop('user_id',axis=1,inplace=True)\n","revTest.drop('date',axis=1,inplace=True)\n","revTest\n","\n","dfTrain = pd.read_csv(\"X_trainToronto.csv\")\n","dfTest = pd.read_csv(\"X_testToronto.csv\")\n","\n","dfTrain['categories'] = dfTrain['categories'].astype(str)\n","dfTrain.set_index('business_id',inplace=True)\n","dfTest.set_index('business_id',inplace=True)\n"]},{"cell_type":"code","execution_count":215,"metadata":{},"outputs":[],"source":["# Análise de sentimentos das reviews\n","# Aqui a ideia é gerar um novo dataframe com a estimativa de sentimento para cada review\n","# Esse novo dataframe será salvo em um novo arquivo csv para facilitar os testes, já que demorar mais de 10 minutos para rodar.\n","\n","stop_words = stopwords.words('english')\n","stemmer = PorterStemmer()\n","stopwords_dict = Counter(stop_words)\n","PONTUACAO = string.punctuation\n","\n","def remove_pontuacao(text):\n","    return text.translate(str.maketrans('', '', PONTUACAO))\n","def remove_stopwords(text):\n","    return ' '.join([word for word in text.split() if word not in stopwords_dict])\n","def stem_words(text):\n","    return \" \".join([stemmer.stem(word) for word in text.split()])\n","def remove_url(text):\n","    return re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n","\n","def preprocess(dfToProcess,filename):\n","    dfTexts = dfToProcess['text']\n","    dfTexts = dfTexts.str.lower()\n","\n","    #Com o preprocessamento o score diminiui, optei por analisar o sentimento do texto puro\n","    \n","    \"\"\" dfTexts = dfTexts.apply(lambda text: remove_stopwords(text))\n","    dfTexts = dfTexts.apply(lambda text: remove_pontuacao(text))\n","    dfTexts= dfTexts.apply(lambda text: stem_words(text)) \"\"\"\n","    \n","    sent = SentimentIntensityAnalyzer()\n","    dfTexts = pd.DataFrame(dfTexts)\n","    dfTexts.columns = ['text']\n","    dfTexts['result'] =  [sent.polarity_scores(b) for b in dfTexts['text']]\n","\n","    # a função polarity_scores retorna um dicionário, ele será armazenado em uma coluna\n","    # e na sequência ele será tratado e dividido em 4 novas colunas no dataframe ('neg','neu','pos','compound')\n","    dfProcessed = pd.concat([dfToProcess.loc[:,['business_id']], pd.json_normalize(dfTexts['result']),dfTexts['text']],axis=1) \n","\n","    if filename:\n","        dfProcessed.to_csv(filename)\n","    return dfProcessed\n","\n"]},{"cell_type":"code","execution_count":214,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["                           neg       neu     pos  compound  destaque\n","business_id                                                         \n","zzvlwkcNR1CCqOPXwuvz2A  0.0475  0.820125  0.1325  0.477787         0\n","                             neg       neu       pos  compound\n","business_id                                                   \n","zzUj3ej4vm_DtvRxNvWDEw  0.053048  0.788563  0.158389  0.561529\n"]}],"source":["\n","#Gera o dataframe com a analaise de sentimento\n","\"\"\" dfTextTrain = preprocess(revTrain,'revTrainNotProcessada.csv')\n","dfTextTest = preprocess(revTest,'revTestNotProcessada') \"\"\"\n","\n","# carrega na memoria o arquivo gerado\n","dfTextTrain = pd.read_csv('revTrainNotProcessada.csv')\n","dfTextTest= pd.read_csv('revTestNotProcessada.csv')\n","\n","try:\n","    dfTextTest.drop(['Unnamed: 0'],axis=1,inplace=True)\n","except:\n","    pass\n","\n","try:\n","    dfTextTrain.drop(['Unnamed: 0'],axis=1,inplace=True)\n","except:\n","    pass\n","\n","#Aqui os resultados da análise de sentimentos são agrupados por estabelicimento e pela média dos valores de cada estabelicimento\n","dfTextTrain = dfTextTrain.groupby('business_id').mean()\n","dfTextTest = dfTextTest.groupby('business_id').mean() \n","\n","dfTextTrain = pd.concat([dfTextTrain,dfTrain['destaque']],axis=1)\n","print(dfTextTrain.tail(1))\n","print(dfTextTest.tail(1))\n"]},{"cell_type":"code","execution_count":276,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0.814872433105165\n","0.814872433105165\n","{'n_neighbors': 64, 'weights': 'distance'}\n","KNeighborsClassifier(n_neighbors=64, weights='distance')\n"]}],"source":["#Utilizando o GridSearchCV para encontrar os melhos parametros para o KNN\n","X =  dfTextTrain.loc[:,['neg','pos','compound']]\n","y = dfTextTrain['destaque']\n","\n","k_range=range(1, 100)\n","weight_options = ['uniform', 'distance']\n","param_grid = dict(n_neighbors=k_range, weights=weight_options)\n","\n","grid = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5, scoring='accuracy',n_jobs = -1).fit(X, y)\n","print(f'{grid.best_score_}')\n","\n","print(grid.best_score_)\n","print(grid.best_params_)\n","print(grid.best_estimator_)"]},{"cell_type":"code","execution_count":265,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["KNeighborsClassifier: 0.81487\n","NaiveBayes: 0.76509\n","DecisionTreeClassifier: 0.74207\n"]}],"source":["\n","\n","X =  dfTextTrain.loc[:,['neg','pos','compound']]\n","y = dfTextTrain['destaque']\n","\n","neigh = KNeighborsClassifier(n_neighbors=64, weights='distance')\n","\n","scores = (cross_val_score(neigh, X, y, cv=5)).mean().round(5)\n","print(f'KNeighborsClassifier: {scores}')\n","\n","naiveBayes = GaussianNB()\n","scores = (cross_val_score(naiveBayes, X, y, cv=5)).mean().round(5)\n","print(f'NaiveBayes: {scores}')\n","\n","tree_model = tree.DecisionTreeClassifier()\n","modelo = tree_model.fit(X,y)\n","scores = (cross_val_score(modelo, X, y, cv=5)).mean().round(5)\n","print(f'DecisionTreeClassifier: {scores}')\n","\n"]},{"cell_type":"code","execution_count":275,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["AdaBoostClassifier: 0.8135\n","MLPClassifier: 0.81462\n"]}],"source":["\n","X =  dfTextTrain.loc[:,['neg','pos','compound']]\n","y = dfTextTrain['destaque']\n","\n","\n","ada = AdaBoostClassifier(n_estimators=100, random_state=22)\n","scores = (cross_val_score(ada, X, y, cv=5)).mean().round(5)\n","print(f'AdaBoostClassifier: {scores}') \n","\n","MLPC = MLPClassifier(solver='adam',alpha=1e-5,random_state=22)\n","\n","\n","scores = (cross_val_score(MLPC, X, y, cv=5)).mean().round(5)\n","print(f'MLPClassifier: {scores}')\n","\n","\n"]},{"cell_type":"code","execution_count":271,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>res1</th>\n","      <th>res2</th>\n","      <th>res3</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>2831</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2832</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2833</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2834</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2835</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2836 rows × 3 columns</p>\n","</div>"],"text/plain":["      res1  res2  res3\n","0        0     0     0\n","1        1     1     1\n","2        0     0     0\n","3        1     1     1\n","4        1     1     1\n","...    ...   ...   ...\n","2831     0     0     0\n","2832     0     0     0\n","2833     0     0     0\n","2834     0     0     0\n","2835     0     0     0\n","\n","[2836 rows x 3 columns]"]},"execution_count":271,"metadata":{},"output_type":"execute_result"}],"source":["\n","neighModel = neigh.fit(X, y)\n","resultado1 = neighModel.predict(dfTextTest.loc[:,['neg','pos','compound']])\n","\n","modelMLPC = MLPC.fit(X, y)\n","resultado2 = modelMLPC.predict(dfTextTest.loc[:,['neg','pos','compound']])\n","\n","ModelAda = ada.fit(X, y)\n","resultado3 = ModelAda.predict(dfTextTest.loc[:,['neg','pos','compound']])\n","\n","res = pd.DataFrame({'res1':resultado1,'res2':resultado2,'res3':resultado3})\n","res"]},{"cell_type":"code","execution_count":273,"metadata":{},"outputs":[{"data":{"text/plain":["business_id\n","-03HVYxkeYWaafEpNJo1SA    0\n","-0M3o2uWBnQZwd3hmfEwuw    1\n","-0aOudcaAyac0VJbMX-L1g    0\n","-1NPtXJaDSoqxQPxt3rg3Q    1\n","-2TBP3ZGu7M-FmfoNJvbrQ    1\n","                         ..\n","zrQlLZLhToH4IHi8jCcbdQ    0\n","zwGu5H7MnilB9Fw7DZ7kXQ    0\n","zxU67OiGi8sF8zbdlTU0mQ    0\n","zzMcX99BPGSOFMZ4boINSQ    0\n","zzUj3ej4vm_DtvRxNvWDEw    0\n","Name: destaque, Length: 2836, dtype: int32"]},"execution_count":273,"metadata":{},"output_type":"execute_result"}],"source":["\n","#sistema de votação por maioria gerando a previsão de destaque, soma todos os resultados se o for maior ou igual a dois a maioria \"votou\" por destaque, caso contrario nao tem destaque\n","dfTextTest['destaque'] = np.where(res['res1'] + res['res2'] + res['res3'] >= 2 , 1, 0)\n","dfTextTest['destaque'].to_csv('resultado.csv')\n","dfTextTest['destaque']"]},{"cell_type":"code","execution_count":268,"metadata":{},"outputs":[{"data":{"text/plain":["business_id\n","-03HVYxkeYWaafEpNJo1SA    0\n","-0M3o2uWBnQZwd3hmfEwuw    1\n","-0aOudcaAyac0VJbMX-L1g    0\n","-1NPtXJaDSoqxQPxt3rg3Q    1\n","-2TBP3ZGu7M-FmfoNJvbrQ    1\n","                         ..\n","zrQlLZLhToH4IHi8jCcbdQ    0\n","zwGu5H7MnilB9Fw7DZ7kXQ    0\n","zxU67OiGi8sF8zbdlTU0mQ    0\n","zzMcX99BPGSOFMZ4boINSQ    0\n","zzUj3ej4vm_DtvRxNvWDEw    0\n","Name: destaque, Length: 2836, dtype: int64"]},"execution_count":268,"metadata":{},"output_type":"execute_result"}],"source":["# Utilizando o KNN para prever o destaque do dataset de teste\n","neighModel = neigh.fit(X, y)\n","resultado = neighModel.predict(dfTextTest.loc[:,['neg','pos','compound']])\n","\n","dfTextTest['destaque'] = resultado\n","dfTextTest['destaque'].to_csv('resultado.csv')\n","dfTextTest['destaque']"]}],"metadata":{"interpreter":{"hash":"5ee95b029e4ab5c0a06afb12c9bd39f8a13a7944dffc909fa00149d86cb79300"},"kernelspec":{"display_name":"Python 3.9.6 64-bit (system)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
